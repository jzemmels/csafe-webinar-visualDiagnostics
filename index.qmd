---
title: "Visual Diagnostics for Algorithmic Cartridge Case Comparisons"
author: "Joseph Zemmels, Susan VanderPlas, Heike Hofmann"
title-slide-attributes: 
  data-background-image: images/title-slide-bkgd.png
  data-background-size: contain
bibliography: refs.bib
format: 
  revealjs
---

## Acknowledgements

```{r setup,include=FALSE}
library(x3ptools)
library(tidyverse)
library(rgl)
library(impressions)
library(patchwork)

knitr::opts_chunk$set(fig.align = "center")
```


**Funding statement**

This work was partially funded by the Center for Statistics and Applications in Forensic Evidence (CSAFE) through Cooperative Agreement 70NANB20H019 between NIST and Iowa State University, which includes activities carried out at Carnegie Mellon University, Duke University, University of California Irvine, University of Virginia, West Virginia University, University of Pennsylvania, Swarthmore College and University of Nebraska, Lincoln.

::: {.notes}

Thank you for that introduction and thanks to you all for being here

My name is Joe and I today will discuss some of the research we've done on comparing cartridge case evidence.

If you have any questions, feel free to type them in chat as we go and I will get to them at the end of the presentation.

:::

# Background

::: {.notes}

I'm sure that many of you are already familiar with cartridge case comparisons, but I am going to start off with some background information to make sure that we're all on the same playing field.

:::

## Cartridge Case Comparisons {.smaller}

-   Determine whether two cartridge cases were fired from the same firearm.

```{r,fig.align='center',out.width="35%"}
knitr::include_graphics("images/gunFiringAnimation.gif")
```


- **Cartridge Case**: metal casing containing primer, powder, and projectile

- **Breech Face**: back wall of gun barrel

- **Breech Face Impressions**: markings left on cartridge case surface by the breech face during the firing process

::: {.aside}

GIF source: <https://imgur.com/4CXf9BK>

:::

::: {.notes}

For  this project, we're interested in determining whether two cartridge cases were fired from the same firearm.

A cartridge case is a metal casing containing the primer, powder, and projectile.
In the animation, you can see an example of a cartridge case that is ejected from the barrel after firing

When a gun is fired and the bullet travels down the barrel, the cartridge case stays in the barrel and is sent backwards as a reaction to the bullet moving forward.

It then slams against the back wall of the barrel, also known as the "breech face," with great force.

Markings on the breech face are impressed into surface of the cartridge case, and this leaves so-called "breech face impressions."

Forensic examiners use these breech face impressions analogous to a fingerprint to identify the gun from which a cartridge case was fired.

:::

## Current Practice {.smaller}

- Cartridge cases recovered from crime scene vs. fired from suspect's firearm

- Place evidence under a comparison microscope for simultaneous viewing [@Thompson2017]

- Assess the "agreement" of impressions on the two cartridge cases [@AFTE1992]

![](images/cartridgeCaseZoomIn.png){fig-align="center" width=1000}



::: {.notes}

Suppose that you recover two cartridge cases - one from a crime scene and another is from a suspect's firearm

The way that forensic examinations are commonly performed today involves placing the cartridge cases under a comparison microscope.

An example of a comparison microscope is shown at the bottom of the slide.
The examiner would place the two cartridge cases on stages.
You can see that there are two microscopes, one for each stage, that are combined into a single view that the examiner can look through.

The goal of the forensic examination is to assess the "agreement" of the impressions on the two cartridge cases.
For our purposes, we are particularly interested in the impressions on the cartridge case primer, which you can see higlighted on the left side of the diagram

The final result is the examiner's conclusion on whether the cartridge cases originated from the same firearm.

:::

## Impression Comparison Algorithms {.smaller}

@nas2009:

*"[T]he decision of a toolmark examiner remains a subjective decision based on unarticulated standards and no statistical foundation for estimation of error rates"*

. . .

@pcast:

*"A second - and more important - direction is (as with latent print analysis) to convert firearms analysis from a subjective method to an objective method. This would involve developing and testing image-analysis algorithms for comparing the similarity of tool marks on bullets [and cartridge cases]."*

. . .

We discuss an image-analysis algorithm to compare 3D topographical images of cartridge cases

  - Visual diagnostics aid in understanding what the algorithm does "under the hood."

::: {.notes}

Now why are we talking about comparison algorithms today?

Well, in recent years the scientific validity of many forensic disciplines has been called into question.

For example in 2009, a report from the National Research Council stated that the decision of a toolmark examiner, which is the examiner who would be looking at these comparisons, remains a subjective decision based on unartiluated standards and no statistical foundation for estimation of error rates

**NEXT**

Seven years later, the President's Council of Advisors on Science and Technology said something similar and emphasized that firearms analysis should convert from a subjective method to an objective method, which would involve developing image analysis algorithms for comparing the similarity of tool marks on firearm evidence including cartridge cases.

**NEXT**

Today, we will discuss an image-analysis algorithm that compares cartridge case evidence.
In particular, we'll introduce a series of diagnostic tools that are useful in understanding how these algorithms work "under the hood," so to speak

:::

# Cartridge Case Comparison Algorithms

::: {.notes}

Now that we have the basics down, let's discuss algorithms that compare cartridge cases.

:::

## Ames I Study {.smaller}

- @baldwin collected cartridge cases from 25 Ruger SR9 pistols

. . .

- Separated cartridge cases into quartets: 3 *known-match* + 1 *unknown source*

- *Match* if fired from the same firearm,  *Non-match* if fired from different firearms

. . .

- 216 examiners tasked with determining whether the unknown cartridge case originated from the same pistol as the known-match cartridge cases

- @baldwin interested in the "false positive" and "false negative" error rates of these examiners

  - *False Positive*: Classifying a non-match as a match
  
  - *False Negative*: Classifying a match as a non-match


. . .

- 0.80% overall error rate (26 out of 3,268)

- 1.01% false positive rate (22 out of 2,178 comparisons)

- 0.37% false negative rate (4 out of 1,090 comparisons)

::: {.notes}

Before we dive into algorithms, I wanted to first discuss the data that we will be using throughout this presentation.

We use cartridge cases collected as part of a 2014 study, commonly called the "Ames I" study because it included researchers from Iowa State University.

For the study, the researchers fired cartridge cases from 25 Ruger SR9 pistols

**NEXT**

These cartridge cases were then separated into groups of 4 consisting of 3 "known match" cartridge cases and 1 "unknown source" cartridge case.
It's important to note here that we call cartridge cases a "match" if they were fired from a firearm and "non-match" if they were fired from different firearms.

**NEXT**

The researchers then sent these cartridge case sets to 216 examiners who were tasked with determining whether the one unknown source cartridge case came from the same pistol as the three known-match cartridge cases

The researchers were particularly interested in assessing the false positive and false negative error rates of these examiners
A false positive error occurs when a non-match cartridge case is classified as a match while a false negative error occurs when a matching cartridge case is classified as a non-match

**NEXT**

The results they got back showed that the examiners made very few errors during their examinations.
Out of 3,268 comparisons, examiners only got 26 wrong with a false positive rate of 1.01% and false negative rate of 0.37%.

One goal that we had for this project was to come up with an algorithm that had similar performance to the results of this study


:::


## Cartridge Case Data {.smaller}

- 3D topographic images from Cadre TopMatch scanner [@topmatch]

- **x3p** file contains surface measurements at the micrometer ("micron") level

```{r x3pImage,fig.align='center',fig.width=5,eval=TRUE}
# knitr::knit_hooks$set(webgl = hook_webgl)

K013sA1 <- x3p_read("data/K013sA1.x3p")

K013sA1$mask <- NULL

x3p_image(K013sA1 %>% x3p_sample(m = 4) %>% x3p_rotate(angle = 90),zoom = 1.5)
rglwidget()
```

::: {.notes}

So we have these cartridge cases from the Ames I study, but the question is: "how do we go from a physical cartridge case to something that we can use on a computer?"

The answer is to take a 3D topographical scan of the cartridge case surface using the Cadre TopMatch scanner.
You can see example of one such topographic image on the slide.
This scan is taken at the micrometer, or "micron," level and stored in the x3p file format

This image is actually interactable, so I can show you what the scan looks like from different angles.
Again, we are interested in the cartridge case primer, which is the circular region in the middle of the scan.
You can see that we pick up areas around the primer that we will eventually remove.
You will also note that the firing pin impression is a sort of plateaued region in the middle of the primer that is caused by the deformation of the metal when it's struck by the firing pin.

Keep in mind that we are specifically interested in circular region around this firing pin impression.

:::


## Cartridge Case Comparison Algorithms {.smaller}

Obtain an objective measure of similarity between two cartridge cases

- **Step 1**: Independently *pre-process* scans to isolate breech face impressions

. . .

- **Step 2**: *Compare* two cartridge cases to extract a set of numerical features that distinguish between matches vs. non-matches

. . .

- **Step 3**: Combine numerical features into a single similarity *score* (e.g., predicted probability of a match)

. . .

Examiner takes similarity score into account during an examination

Challenging to know how/when these steps work correctly

::: {.notes}

Now let's discuss comparison algorithms

The goal of a cartridge case comparison algorithm is to obtain a measure of similarity between two cartridge cases

There are different types of algorithms out there right now, but they all follow the same, basic structure

The first step of these algorithms is to pre-process the scans to isolate the breech face impressions.
For example, in the scan we talked about on the last slide, we want to remove the firing pin impression and region around the primer to isolate the breech face impressions.

**NEXT**

Next, once we have two pre-processed cartridge cases, we compare them to extract a set of numerical features that distinguish between matches and non-matches
I'll talk in a few slides about some common numerical features we calculate

**NEXT**

Finally, we combine the numerical features into a single similarity score such as the predicted probability that the two cartridge cases match.

**NEXT**

Eventually, we hope that these algorithms will be used in casework to help the examiner make a conclusion
However, we first need to understand an algorithm's limitations before we know *how* the examiner should interpret the similarity score

One challenge we've commonly faced while working with these comparison algorithms is knowing how and when these steps work as we intend them to.
In the next few slides, I will discuss challenges we've faced at each step.

:::

## Step 1: Pre-process {.smaller}

Isolate region in scan that consistently contains breech face impressions

```{r,fig.width=8,fig.align='center',eval=TRUE,include=FALSE}
K013sA1 <- x3p_read("data/K013sA1.x3p") %>%
  x3p_rotate(angle = 180) %>%
  x3p_flip_y() %>%
  sample_x3p(m = 4)

K013sA1$mask <- NULL

K013sA1_processed <- x3p_read("data/K013sA1_processed.x3p") %>%
  x3p_rotate(angle = 180) %>%
  x3p_flip_y() %>%
  x3p_rotate(angle = 90) %>%
  sample_x3p(m = 4)
K013sA1_processed$surface.matrix <- K013sA1_processed$surface.matrix*1e6
K013sA1_processed$header.info$incrementY  <- K013sA1_processed$header.info$incrementY*1e6
K013sA1_processed$header.info$incrementX  <- K013sA1_processed$header.info$incrementX*1e6

K013sA1$surface.matrix <- K013sA1$surface.matrix %>%
  imager::as.cimg() %>%
  imager::pad(nPix = 1,axes = "x",val = 100,pos = -1) %>%
  as.matrix()

K013sA1_processed$surface.matrix <- K013sA1_processed$surface.matrix %>%
  imager::as.cimg() %>%
  imager::pad(nPix = nrow(K013sA1$surface.matrix) - nrow(K013sA1_processed$surface.matrix),
              axes = "x",val = 100) %>%
  imager::pad(nPix = ncol(K013sA1$surface.matrix) - ncol(K013sA1_processed$surface.matrix),
              axes = "y",val = 100) %>%
  as.matrix()

K013sA1_processed$surface.matrix[K013sA1_processed$surface.matrix == 100] <- NA


K013sA1_combined <- K013sA1_processed

K013sA1_combined$surface.matrix <- rbind(K013sA1$surface.matrix,matrix(NA,ncol = ncol(K013sA1$surface.matrix),nrow = 10),K013sA1_processed$surface.matrix)
```

```{r,fig.width=8,fig.align='center',eval=TRUE,include=TRUE}
x3p_image(K013sA1_combined,zoom=1.5)

# x3p_snapshot(file = "figures/preProcess_x3pImage.png")
# knitr::plot_crop("figures/preProcess_x3pImage.png")
# rgl::close3d()

rglwidget()
```


<!--  ```{r,out.width="100%",include=TRUE,eval=TRUE} -->
<!--  knitr::include_graphics("figures/preProcess_x3pImage.png") -->
<!--  ``` -->


<!-- ```{r preProcessExample,fig.align='center',eval=FALSE} -->
<!-- if(!file.exists("figures/preProcessExample.png")){ -->
<!--   K013sA1 <- x3p_read("data/K013sA1.x3p") %>% -->
<!--     x3p_rotate(angle = 180) %>% -->
<!--     x3p_flip_y() -->

<!--   K013sA1_processed <- x3p_read("data/K013sA1_processed.x3p") -->
<!--   K013sA1_processed$surface.matrix <- K013sA1_processed$surface.matrix*1e6 -->

<!--   plt1 <- impressions::x3pPlot(K013sA1, -->
<!--                                x3pNames = c("K013sA1"), -->
<!--                                legend.quantiles = c(0,.1,.5,.9,1)) -->

<!--   plt2 <- impressions::x3pPlot(K013sA1_processed, -->
<!--                                x3pNames = c("K013sA1 Pre-processed"), -->
<!--                                legend.quantiles = c(0,.1,.5,.9,1)) -->

<!--   plt <- (plt1 | plt2) -->

<!--   ggsave(plot = plt,filename = "figures/preProcessExample.png",width = 10,height = 5) -->
<!--   knitr::plot_crop("figures/preProcessExample.png") -->
<!-- } -->
<!-- ``` -->

<!-- ```{r,out.width="100%",include=TRUE,eval=FALSE} -->
<!-- knitr::include_graphics("figures/preProcessExample.png") -->
<!-- ``` -->

. . .

***How do we know when a scan is adequately pre-processed?***

::: {.notes}

The purpose of the pre-process step is to isolate the region of the scan that consistently contains breech face impressions.

In our case, this is the circular region of the primer around the firing pin impression.
Again, the example you see here is interactable, so I can show you that we end up removing a lot of the scan to isolate the region of interest.

**NEXT**

One issue that we commonly ran into while pre-processing scans was knowing when scans were adequately pre-processed.
You know, what does it even mean for pre-processing to be "adequate?"
I'll talk about we addressed this question in a few slides.

:::

## Step 2: Compare Full Scans {.smaller}

- *Registration*: Determine rotation and translation to align two scans


```{r fullScanComparison,eval=FALSE}
K013sA1_processed <- x3p_read("data/K013sA1_processed.x3p") %>%
  x3p_rotate(angle = 180) %>%
  x3p_flip_y() %>%
  x3p_rotate(angle = 90)

K013sA2_processed <- x3p_read("data/K013sA2_processed.x3p") %>%
  x3p_rotate(angle = 180) %>%
  x3p_flip_y() %>%
  x3p_rotate(angle = 90)

x3p_image(K013sA1_processed,zoom=.7)
# x3p_snapshot("figures/K013sA1_processed_x3pImage.png")
# knitr::plot_crop("figures/K013sA1_processed_x3pImage.png")
rgl::close3d()

x3p_image(K013sA2_processed,zoom=.7)
# x3p_snapshot("figures/K013sA2_processed_x3pImage.png")
# knitr::plot_crop("figures/K013sA2_processed_x3pImage.png")
rgl::close3d()
# 
# if(!file.exists("data/fullScan_knownMatch.rds")){
#   fullScan_knownMatch <- scored::comparison_fullScan(reference = K013sA1_processed,
#                                           target = K013sA2_processed,
#                                           returnX3Ps = FALSE) %>%
#     group_by(direction) %>%
#     filter(fft_ccf == max(fft_ccf)) %>%
#     ungroup() %>%
#     select(direction,theta) %>%
#     pmap_dfr(~ {
#       
#       scored::comparison_fullScan(reference = K013sA1_processed,
#                                   target = K013sA2_processed,
#                                   thetas = ..2,
#                                   returnX3Ps = TRUE) %>%
#         filter(direction == ..1)
#       
#     })
#   
#   saveRDS(fullScan_knownMatch,"data/fullScan_knownMatch.rds")
# }
```

```{r,fig.align='center',out.width="75%"}
knitr::include_graphics("images/fullScanRegistrationDiagram_x3pImage.png")
```


<!-- ```{r,fig.align='center',out.width="75%"} -->
<!-- knitr::include_graphics("images/fullScanRegistrationDiagram.png") -->
<!-- ``` -->

. . .

- *Cross-correlation function* (CCF) measures similarity between scans

  - Choose the rotation/translation that maximizes the CCF


::: {.notes}

The next step is to compare two pre-processed scans.

A common technique we use to compare two scans is called "registration," which essentially involves finding the rotation and translation at which the two scans align best.

In the example on the slide, we have two matching scans K013sA1 and K013sA2 that fired from the same firearm.
To "register" the two scans, we need to slightly rotate and shift one of the scans to align to the other.

**NEXT**

The way we often choose a registration is by using the Cross-Correlation Function, which measures the similarity between two scans.
A large CCF value implies highly similar scans.
As such, we choose the rotation and translation that maximizes the cross-correlation function between the two scans.

One important note is that cartridge cases often only have a few regions with distinguishable impressions.
This means that even for matching scans, the cross-correlation may not be very large since similarities are "drowned-out" by the dissimilarities.

**NEXT SLIDE**

:::

## Step 2: Compare Cells {.smaller}

- Split one scan into a grid of cells that are each registered to the other scan [@song_proposed_2013]

- For a matching pair, we assume that cells will agree on the same rotation & translation

```{r,eval=FALSE}
if(!file.exists("data/cellBased_knownMatch.rds")){
  
  K013sA1_processed <- x3p_read("data/K013sA1_processed.x3p")
  K013sA2_processed <- x3p_read("data/K013sA2_processed.x3p")
  
  cellBased_knownMatch <- bind_rows(scored::comparison_cellBased(reference = K013sA1_processed,
                                                     target = K013sA2_processed,
                                                     thetas = 3,
                                                     numCells = c(8,8),
                                                     direction = "one",
                                                     returnX3Ps = TRUE) %>%
                          mutate(direction = "reference_vs_target"),
                        scored::comparison_cellBased(reference = K013sA2_processed,
                                                     target = K013sA1_processed,
                                                     thetas = -3,
                                                     numCells = c(8,8),
                                                     direction = "one",
                                                     returnX3Ps = TRUE) %>%
                          mutate(direction = "target_vs_reference"))
  
  saveRDS(cellBased_knownMatch,"data/cellBased_knownMatch.rds")
}

cellBased_knownMatch %>%
  filter(direction == "reference_vs_target") %>%
  select(cellHeightValues,alignedTargetCell,cellIndex) %>%
  pmap(~ {
    
    x3pPlot(..1,..2,x3pNames = c(..3,"."))
    
  })
```

```{r,fig.align='center',out.width="75%"}
knitr::include_graphics("images/cellBasedRegistrationDiagram_x3pImage.png")
```


```{r,fig.align='center',out.width="75%",eval=FALSE}
knitr::include_graphics("images/cellBasedRegistrationDiagram.png")
```

. . .

***Why does the algorithm "choose" a particular registration?***

::: {.notes}

To solve this issue, John Song, a researcher at NIST, proposed splitting one of the cartridge cases into a grid of "cells," each of which are registered in the other scan.
So we essentially repeat the process from the last slide, but now for each cell.
This allows us to consider specific regions of the scan that might contain distinguishable markings rather than considering the full scans all at once.

The key assumption we make here is that cells will agree on the same rotation and translation if the cartridge case pair is truly matching.
In the example on the slide, you see three cells from the scan on the left and where they register in the other scan.
One thing I will point out here is that the two cells in the top-right appear to agree on the same registration, as evidenced by these parallel connecting lines.
In contrast, the cell in the bottom left does not agree with the registration -- you see that the connecting line is not parallel to the other two.

**NEXT**

So something we wrestled with at this step of the algorithm is understanding why the algorithm "chooses" a particular registration.
In a few slides, I will discuss tools we developed to address this question.

:::

## Step 3: Score {.smaller}

- Measure of similarity for two cartridge cases

  - Maximized CCF (0.27 in example below) [@vorburger_surface_2007; @tai_fully_2018]

  - Congruent Matching Cells (11 CMCs in example below) [@song_proposed_2013]
  
```{r}
if(!file.exists("figures/cmcPlot_knownMatch.png")){
  
  K013sA1_processed <- x3p_read("data/K013sA1_processed.x3p")
  K013sA2_processed <- x3p_read("data/K013sA2_processed.x3p")
  
  cellBasedComparison_8x8 <- scored::comparison_cellBased(reference = K013sA1_processed,
                                                          target = K013sA2_processed,
                                                          direction = "both",
                                                          numCells = c(8,8),
                                                          returnX3Ps = FALSE)
  
  
  cmcClassifs <- cellBasedComparison_8x8 %>%
    group_by(direction) %>%
    mutate(originalMethod = cmcR::decision_CMC(cellIndex=cellIndex,
                                               x=x,
                                               y=y,
                                               theta=theta,
                                               corr=pairwiseCompCor))
  
  cmcs <- cmcClassifs %>%
    filter(originalMethod == "CMC") %>%
    filter(direction == "reference_vs_target" & originalMethod == "CMC") %>%
    ungroup() %>%
    select(cellIndex,theta,originalMethod)
  
  non_cmcs <- cmcClassifs %>%
    filter(direction == "reference_vs_target") %>%
    group_by(cellIndex) %>%
    filter(fft_ccf == max(fft_ccf)) %>%
    ungroup() %>%
    select(cellIndex,theta,originalMethod) %>%
    anti_join(cmcs,by = "cellIndex")
  
  alignedCells <- bind_rows(cmcs,non_cmcs) %>%
    group_by(theta) %>%
    group_split() %>%
    map_dfr(function(dat){
      
      scored::comparison_cellBased(reference = K013sA1_processed,
                                   target = K013sA2_processed,
                                   direction = "one",
                                   numCells = c(8,8),
                                   thetas = unique(dat$theta),
                                   returnX3Ps = TRUE) %>%
        filter(cellIndex %in% dat$cellIndex) %>%
        left_join(dat %>% select(cellIndex,originalMethod),
                  by = "cellIndex")
      
    })
  
  saveRDS(alignedCells,file = "data/cellBased_knownMatch_8x8.rds")
  
  cmcPlot_knownMatch <- cmcR::cmcPlot(reference = K013sA1_processed,
                                      target = K013sA2_processed,
                                      cmcClassifs = alignedCells)
  
  ggsave(filename = "figures/cmcPlot_knownMatch.png",plot = cmcPlot_knownMatch,height = 5,width = 10)
  knitr::plot_crop("figures/cmcPlot_knownMatch.png")
  
}
```


```{r,include=TRUE,out.width=600}
knitr::include_graphics("figures/cmcPlot_knownMatch.png")
```

. . .

- **Our approach**: predicted probability of a match using a statistical model

***What factors influence the final similarity score?***


::: {.notes}

Once we compare the two scans, the final step is to return some similarity score.
Different scores have been proposed over the years.

For example, the maximum cross-correlation value is a reasonable choice since it measures the similarity of the two scans.
However, we talked about the CCF may not be very large if we compute the maximum CCF between the full scans.
In the example on the slide, the maximum CCF is 0.27, which isn't very large given that the two cartridge cases we've been working with are matching.

Another proposed similarity score proposed by John Song in 2013 is the number of "Congruent Matching Cells" or CMCs.
This CMC algorithm essentially tries to identify cells that "agree" on the same registration.
In the example on the slide, you see 11 CMCs in blue and 28 non-CMCs in red.
Notice that blue cells are organized in a grid-like pattern as we would expect.
For example, we expect cell 2, 7 to be to the right of cell 2, 6 and this indeed what we see in the second scan.
On the other hand, cell 2, 5 is not at all where we would expect, so it makes sense that we would exclude it from the similarity score.


**NEXT**

We take a slightly different approach by using a statistical model to predict the probability that the two cartridge cases match.

As we worked with these similarity scores, it became clear that we did not understand the factors that influenced the final result.
We were sort of at the whim of the algorithm.
Again, I will discuss some tools that we created to address this challenge.

:::

# Visual Diagnostics

::: {.notes}

Now let's discuss some of the visual diagnostic tools we developed.

:::

## Visual Diagnostics for Algorithms {.smaller}

- A number of questions arise out of using comparison algorithms

  - *How do we know when a scan is adequately pre-processed?*

  - *Why does the algorithm "choose" a particular registration?*

  - *What factors influence the final similarity score?*


. . .

- We wanted to create tools to address these questions

  - Well-constructed visuals are intuitive and persuasive

  - Useful for both researchers and practitioners to understand the algorithm's behavior


::: {.notes}

After working with the comparison algorithms for a while, we naturally developed a set of questions related to understanding how the algorithms worked.

**NEXT**

We were interested in creating diagnostic tools to address these questions.
A natural choice for us was visual diagnostics
This is because well-constructed visuals can simultaneously be approachable while providing complex, nuanced insights.

The diagnostic tools we developed have already been useful for us as researchers and we hope that they will prove useful to practitioners who are interested in understanding the algorithm's behavior.

:::

## X3P Plot {.smaller}

```{r x3pPlot-comparison}
if(!file.exists("figures/x3pPlot_comparison.png")){
  
  K013sA1_processed <- x3p_read("data/K013sA1_processed.x3p")
  K013sA1_processed$surface.matrix <- K013sA1_processed$surface.matrix*1e6
  K013sA2_processed <- x3p_read("data/K013sA2_processed.x3p")
  K013sA2_processed$surface.matrix <- K013sA2_processed$surface.matrix*1e6
  
  plt <- x3pPlot(K013sA1_processed,K013sA2_processed,x3pNames = c("K013sA1","K013sA2"),legend.quantiles = c(0,.01,.1,.5,.9,.99,1))
  
  ggsave(filename = "figures/x3pPlot_comparison.png",plot=plt,width=10,height = 5,bg="white")
  knitr::plot_crop("figures/x3pPlot_comparison.png")
  
}
```

```{r,include=TRUE,out.width="65%"}
knitr::include_graphics("figures/x3pPlot_comparison.png")
```

- Emphasizes extreme values in scan that may need to be removed during pre-processing

- Allows for comparison of multiple scans on the same color scheme

- Map quantiles of surface values to a divergent color scheme

```{r,include=TRUE,width = "60%"}
knitr::include_graphics("images/x3pPlot_colorscheme.png")
```


::: {.notes}

The first diagnostic tool we will discuss is the X3P plot, which is a way of representing the surface values of cartridge case scans using a purple, white, orange color scheme.

In the example on the slide, you can see two matching cartridge cases.
Purple observations are associated with surface values below the median value while orange observations are above the median. We represent the median value with the color white.

We use this color scheme to emphasize extreme values in the scan and to compare the surface values across scans.
Extreme values are often highly influential in the comparison step, so it is important that we remove extreme values that aren't associated with breech face impressions so that the breech face impressions can be properly compared.

In the example, we can make out some similar markings on the two scans, such as the orange scratch at the 7 o'clock position firing pin impression or the striped impressions around the 4 o'clock position.
There are also noticable differences such as the dent-like marking near the 11 o'clock position of the firing pin impression in the right scan that isn't shared with the left scan.

The X3P plot is useful by itself to compare markings between two scans, but we have found it particularly useful for determining whether a cartridge case requires additional pre-processing.

**NEXT SLIDE**

:::

## X3P Plot Pre-processing Example {.smaller}

- Useful for diagnosing when scans need additional pre-processing

```{r,include=TRUE,fig.align='center',out.width = "50%"}
knitr::include_graphics("images/preProcessEffectExample.png")
```

::: {.notes}

As a concrete example of this, consider a matching pair of cartridge cases I came across while working with the comparison algorithms.
I noticed these two scans because they had uncharacteristically low similarity scores for a matching pair.

The first row shows the two scans after some pre-processing.
You'll notice that the scan on the left contains a lot of extreme values that are uncharacteristic of breech face impressions.
For example, we see that the firing pin impression wasn't entirely removed during the first pass of pre-processing.
There are also large dent-like markings on both scans, such as the purple markings [here] and [here].

These observations are so extreme that they suppress other markings on the two scans that actually look quite similar.
For example, we see faint striped markings on the top of the two scans.
The maximized cross correlation value for these scans is 0.14, which low for a matching pair.

Now compare this to the bottom row, which shows the same two scans after removing the extreme values.
It's a lot easier to see similarities between the two scans - for example those striped impressions are a deeper shade of purple of orange.
The maximized cross-correlation for *these* scans is 0.29.
This CCF value is still low but is much higher than before.

This goes to show how the X3P plot is useful for identifying when scans need additional pre-processing.

:::

## Comparison Plot {.smaller .scrollable}

- Separate aligned scans into similarities and differences

- Useful for understanding a registration

. . .

- *Similarities*: Element-wise average between two scans after filtering elements that are less than 1 micron apart

```{r eval=FALSE}
fullScan_knownMatch <- readRDS("data/fullScan_knownMatch.rds")

reference <- fullScan_knownMatch$cellHeightValues[[1]]
reference$surface.matrix <- reference$surface.matrix*reference$cmcR.info$scaleByVal*1e6

target <- fullScan_knownMatch$alignedTargetCell[[1]]
target$surface.matrix <- target$surface.matrix*target$cmcR.info$scaleByVal*1e6

x3pPlot(impressions::x3p_elemAverage(reference,target),
        x3pNames = "Element-wise Average")

x3pDiff <- reference
x3pDiff$surface.matrix <- abs(reference$surface.matrix - target$surface.matrix)

surfaceMat_df <- purrr::pmap_dfr(.l = list(list(x3pDiff),
                                               "Element-wise Distance"),
                                     function(x3p,name){

                                       x3p$header.info$incrementX <- 1
                                       x3p$header.info$incrementY <- 1
                                       x3p$mask <- NULL

                                       x3p %>%
                                         x3ptools::x3p_to_df() %>%
                                         dplyr::mutate(xnew = max(.data$y) - .data$y,
                                                       ynew = max(.data$x) - .data$x) %>%
                                         dplyr::select(-c(.data$x,.data$y)) %>%
                                         dplyr::rename(x=.data$xnew,
                                                       y=.data$ynew) %>%
                                         dplyr::mutate(x3p = rep(name,times = nrow(.)))
                                     })

surfaceMat_df %>%
      ggplot2::ggplot(ggplot2::aes(x = .data$x,y = .data$y)) +
      ggplot2::geom_raster(ggplot2::aes(fill = .data$value))  +
      ggplot2::scale_fill_gradientn(colours =  c('#2d004b','#542788','#8073ac','#b2abd2','#d8daeb','#f7f7f7','#fee0b6','#fdb863','#e08214','#b35806','#7f3b08'),
                                    values = scales::rescale(quantile(surfaceMat_df$value,
                                                                      c(0,.01,.025,.1,.25,.5,.75,0.9,.975,.99,1),
                                                                      na.rm = TRUE)),
                                    breaks = function(lims){
                                      dat <- quantile(surfaceMat_df$value,c(0,.75,.99,1),na.rm = TRUE)

                                      dat <- dat %>%
                                        setNames(paste0(names(dat)," [",round(dat,3),"]"))

                                      return(dat)
                                    },
                                    na.value = "gray65") +
      ggplot2::coord_fixed(expand = FALSE) +
      ggplot2::theme_minimal() +
      ggplot2::theme(
        axis.title.x = ggplot2::element_blank(),
        axis.text.x = ggplot2::element_blank(),
        axis.ticks.x = ggplot2::element_blank(),
        axis.title.y = ggplot2::element_blank(),
        axis.text.y = ggplot2::element_blank(),
        axis.ticks.y = ggplot2::element_blank(),
        panel.grid.major = ggplot2::element_blank(),
        panel.grid.minor = ggplot2::element_blank(),
        panel.background = ggplot2::element_blank()) +
      ggplot2::guides(fill = ggplot2::guide_colourbar(barheight = grid::unit(3,"in"),
                                                      label.theme = ggplot2::element_text(size = 8),
                                                      title.theme = ggplot2::element_text(size = 10),
                                                      frame.colour = "black",
                                                      ticks.colour = "black"),
                      colour = 'none') +
      ggplot2::labs(fill = expression("Rel. Height ["*mu*"m]")) +
      ggplot2::facet_wrap(~ x3p)


x3pDiff_bin <- x3pDiff
x3pDiff_bin$surface.matrix <- (x3pDiff_bin$surface.matrix > 1)

x3pDiff_bin %>%
  x3ptools::x3p_to_df() %>%
  mutate(x = x/x3pDiff_bin$header.info$incrementX,
         y = y/x3pDiff_bin$header.info$incrementY) %>%
  dplyr::mutate(xnew = max(.data$y) - .data$y,
                                                       ynew = max(.data$x) - .data$x) %>%
                                         dplyr::select(-c(.data$x,.data$y)) %>%
                                         dplyr::rename(x=.data$xnew,
                                                       y=.data$ynew) %>%
  ggplot(aes(x=x,y=y)) +
  geom_raster(fill = "gray65") +
  geom_raster(aes(fill=value)) +
  coord_fixed(expand=FALSE) +
  theme_void() +
  scale_fill_manual(values = c("black","white"),
                    na.value = "gray65",
                    na.translate = FALSE) +
  theme(legend.key = element_rect(color = "black")) +
  labs(fill = "Greater than 1")

x3pAveraged_filt <- x3p_filter(x3p = x3p_elemAverage(reference,target),
                               cond = function(x,y,thresh) abs(y) <= thresh,
                               y = c({reference$surface.matrix - target$surface.matrix}),
                               thresh = 1)

```


![](images/filteringIllustration.png){fig-align="center" width="70%"}

. . .

- *Differences*: Elements of both scans that are at least 1 micron apart

![](images/filteringDifferencesIllustration.png){fig-align="center" width="70%"}

::: {.notes}

Now let's move on to another visual diagnostic tool called the Comparison Plot.

The goal of the comparison plot is to separate two scans into similarities and differences, which is useful for understanding why the algorithm chose a particular registration.

**NEXT**

To compute the similarities, you can imagine overlaying the two surfaces on top of one another.
We then compute the element-wise average and distance between the two scans.

The element-wise average is sort of a hybrid of the two surfaces while the element-wise distance emphasizes where the two surfaces differ the most.

We then apply a filter to the element-wise average based on whether distance between the surfaces is larger than 1 micron.
The plot you see on the slide shows white elements where the distance between the two surfaces is larger than 1.
We remove those elements from the element-wise average, which results in a visualization of the obvious "similarities" between the two original scans.

**NEXT**

Conversely, we define "differences" to be elements of the two scans where the distance is greater than 1 micron.
We apply a filtering to the two original scans, but this time only keep those elements for which the surfaces are far apart.

We combine the similarities and differences into a single visualization to construct the comparison plot

**NEXT SLIDE**

:::

## Full Scan Comparison Plot

```{r}
if(!file.exists("figures/comparisonPlotExample.png")){
  
  fullScan_knownMatch <- readRDS("data/fullScan_knownMatch.rds")
  refAligned <- fullScan_knownMatch$cellHeightValues[[1]]
  refAligned$surface.matrix <- refAligned$surface.matrix*refAligned$cmcR.info$scaleByVal*1e6
  targAligned <- fullScan_knownMatch$alignedTargetCell[[1]]
  targAligned$surface.matrix <- targAligned$surface.matrix*targAligned$cmcR.info$scaleByVal*1e6
  
  plt <- impressions::x3p_comparisonPlot(x3p1 = refAligned,x3p2 = targAligned,
                                         plotLabels = c("K013sA1","K013sA2 Aligned",
                                                        "Element-wise Average",
                                                        "K013sA1 Differences","K013sA2 Differences"),
                                         legendLength = 20,
                                         legendUnit = "micron",
                                         legendQuantiles = c(0,.01,.5,.99,1))
  
  ggsave(filename = "figures/comparisonPlotExample.png",plot = plt,width = 10,height = 6,bg = "white")
  knitr::plot_crop("figures/comparisonPlotExample.png")
  
}
```

```{r,include=TRUE,out.width = "100%"}
knitr::include_graphics("figures/comparisonPlotExample.png")
```

::: {.notes}

An example of a comparison plot of two full scans is shown here.

We show the two scans in the first column after scan K013sA2 has been aligned to K013sA1

The second and third columns show the filtered element-wise average and differences I discussed on the last slide.

To recap, the element-wise average shows the similarities between two scans.
For example, the eye is naturally drawn to some dark orange and purple regions in the bottom of the element-wise average.
After identifying these noteworthy regions, we can go back to the original scans in the first column to further explore the similarities around this area.
For example, the dark purple observations are part of striped markings on the thin edge of the two surfaces.

In a similar manner, we can look at the third column to identify differences between the two scans.
Again, the eye is naturally drawn to the darkest shades.
I see a dark purple region on the bottom of the top-right plot that isn't shared with the bottom-right plot.
If we go back to the first column, we indeed see that these are clear differences in the original scans.
It appears that the deep purple might be part of the firing pin impression that wasn't fully removed from the scan during pre-processing.

We have found a comparison plot for two full scans gives us a clear, high-level idea of the similarities and differences between the two scans.

:::

## Cell Comparison Plot

<!-- ::: {.fragment .fade-up fragment-index=1} -->

<!-- ![](images/cellBasedRegistration_cell1-6.png){.absolute top=0 right=30 width=200 height=100} -->

<!-- ::: -->

```{r}
if(!file.exists("figures/cellBasedComparison.png")){
 
  cellBased_knownMatch <- readRDS("data/cellBased_knownMatch.rds")
  
  plt <- cellBased_knownMatch %>%
    filter(direction == "reference_vs_target") %>%
    filter(cellIndex == "1, 6") %>%
    select(cellIndex,cellHeightValues,alignedTargetCell) %>%
    pmap(~ {
      
      x3p1 <- ..2
      x3p2 <- ..3
      
      x3p1$surface.matrix <- x3p1$surface.matrix*x3p1$cmcR.info$scaleByVal*1e6
      x3p2$surface.matrix <- x3p2$surface.matrix*x3p2$cmcR.info$scaleByVal*1e6
      
      impressions::x3p_comparisonPlot(x3p1,x3p2,
                                      plotLabels = c(paste0("K013sA1 Cell ",..1),
                                                     paste0("K013sA2 Aligned Cell"),
                                                     "Element-wise Average",
                                                     paste0("K013sA1 Cell ",..1," Differences"),
                                                     paste0("K013sA2 Aligned Cell\nDifferences")),
                                      label_y = 45,
                                      legendUnit = "micron")
      
    })
  
  ggsave(filename = "figures/cellBasedComparison.png",plot = plt[[1]],
         width = 10,height = 6,bg = "white")
  knitr::plot_crop("figures/cellBasedComparison.png")
  
}
```

<div class="r-stack">

::: {.fragment fade-out fragment-index=1}

![](images/cellBasedRegistration_cell1-6.png){fig-align="center" width=100%}

:::

::: {.fragment .fade-up fragment-index=1}

```{r,include=TRUE,out.width = "100%"}
knitr::include_graphics("figures/cellBasedComparison.png")
```

:::

</div>

::: {.notes}

However, we have found the comparison plot to be most useful when we use it to compare individual cells.
This is because we are able to zoom into regions of the scan that may not have as much detail in the full scan plot.

For this example, we'll be focusing on the cell in the first row, sixth column of left scan.
We see where it aligns right scan.

**NEXT**

The comparison plot allows us to zoom and get a better idea of the similarities and differences in this region.
Again, we have the original cells in the first column.
In the middle column, our eye is naturally drawn to the dark purple region near the top of the element-wise average.
We see in original scans that this corresponds to similar elliptical dark purple regions.
Further, below that we also see a streak of connected orange observations that look similar between the two scans.
So again, the element-wise average gives us a quick reference to assess the similarities of the two cells

Considering the differences in the third column, we can make a few observations.
For one, we notice that the individual regions here are all relatively small - some only a pixel or two in size.

Further, even the larger regions share some similarities.
For example, take the larger region in the bottom-left of the two plots.
In the top-right plot, we see a descending trend in the height values from the top-left to the bottom-right -- the surface values move from dark to light orange.

We actually the same descending trend in this region in the bottom-right plot, except at a different starting location.
Now, the surface values start at a light orange and move to a light purple.
This discrepancy might happen because of a difference in the amount of pressure applied to the cartridge case surface by the breech face.
These observations might actually be the "same" marking, but one is deeper than the other.
So even the "different" regions between these two cells actually share similarities.

In summary, we've found that "zooming into" regions of a cartridge case scan using the comparison plot is really useful for understanding why the algorithm chose a particular registration.

:::

## Translating Visuals to Statistics {.smaller}

- Translate qualitative observations made about the visual diagnostics into complementary numerical statistics

<!-- ```{r,include=TRUE,out.width = "50%"} -->
<!-- knitr::include_graphics("figures/cellBasedComparison.png") -->
<!-- ``` -->

- Useful to quantify what our intuition says should be true for (non-)matching scans

. . .

- For a matching cartridge case pair...

  1. There should be (many) more similarities than differences
  
  2. The different regions should be relatively small 

  3. The surface values of the different regions should follow similar trends

- Statistics are useful for justifying/predicting the behavior of the algorithm

::: {.notes}

The last set of diagnostic tools I'm going to talk about are actually not visuals.
Instead, they are statistics that we compute based on the visual diagnostics

Throughout this presentation, I've made a lot of qualitative observations about the cartridge case surfaces such as "there are dark purple regions shared between the two scans" or "the differences between the scans are relatively small," etc.

The purpose of these statistics is to translate the qualitative observations we can make about the visual diagnostics into numerical statistics that complement those observations

Using these statistics, we can quantify what our intuition says should be true about matching and non-matching cartridge case pairs

**NEXT**

For example, if we have two truly matching cartridge case pairs, then we can assume...

The number of similarities should outweigh the number of differences (by a lot)

The regions we call "different" should be relatively small and

similar to the observation I made on the last slide, the surface values of the differences should still follow similar trends

As I said before, these statistics provide a numerical complement to the visual diagnostics we discussed before and are useful to predict the behavior of the algorithm.

So we will talk about three statistics that we calculate based on these qualitative observations

:::


## Similarities vs. Differences Ratio {.smaller .scrollable}
  
```{r,eval = FALSE,include=FALSE}
cellBased_knownMatch <- readRDS("data/cellBased_knownMatch.rds")

cell16 <- cellBased_knownMatch %>%
  filter(cellIndex == "1, 6" & direction == "reference_vs_target")

cell16_reference <- cell16$cellHeightValues[[1]]
cell16_target <- cell16$alignedTargetCell[[1]]

cell16_reference$surface.matrix <- cell16_reference$surface.matrix*cell16_reference$cmcR.info$scaleByVal*1e6
cell16_target$surface.matrix <- cell16_target$surface.matrix*cell16_target$cmcR.info$scaleByVal*1e6

x3pAveraged <- x3p_filter(x3p = x3p_elemAverage(cell16_reference,cell16_target),
                            cond = function(x,y,thresh) abs(y) <= thresh,
                            y = c({cell16_reference$surface.matrix - cell16_target$surface.matrix}),
                            thresh = 1)

x3pAveraged_differences <- x3p_filter(x3p = x3p_elemAverage(cell16_reference,cell16_target),
                            cond = function(x,y,thresh) abs(y) > thresh,
                            y = c({cell16_reference$surface.matrix - cell16_target$surface.matrix}),
                            thresh = 1)

x3pPlot(x3pAveraged,x3pAveraged_differences)
```


```{r,eval=FALSE}
cellBased_knownMatch <- readRDS("data/cellBased_knownMatch.rds")

cell16 <- cellBased_knownMatch %>%
  filter(cellIndex == "1, 6" & direction == "reference_vs_target")

cell16_reference <- cell16$cellHeightValues[[1]]
cell16_target <- cell16$alignedTargetCell[[1]]

cell16_reference$surface.matrix <- cell16_reference$surface.matrix*cell16_reference$cmcR.info$scaleByVal*1e6
cell16_target$surface.matrix <- cell16_target$surface.matrix*cell16_target$cmcR.info$scaleByVal*1e6

x3pAveraged_differences <- x3p_filter(x3p = x3p_elemAverage(cell16_reference,cell16_target),
                            cond = function(x,y,thresh) abs(y) > thresh,
                            y = c({cell16_reference$surface.matrix - cell16_target$surface.matrix}),
                            thresh = 1)

x3pAveraged_binarized <- x3pAveraged_differences
x3pAveraged_binarized$surface.matrix <- !is.na(x3pAveraged_binarized$surface.matrix)

imager::label(imager::as.cimg(x3pAveraged_binarized$surface.matrix)) %>%
  as.data.frame() %>%
  filter(value > 0) %>%
  group_by(value) %>%
  tally() %>%
  mutate(n = n*x3pAveraged_binarized$header.info$incrementY*1e6) %>%
  summarize(meanSize = mean(n),
            sdSize = sd(n))

imager::label(imager::as.cimg(x3pAveraged_binarized$surface.matrix)) %>%
  as.data.frame() %>%
  filter(value > 0) %>%
  group_by(value) %>%
  tally() %>%
  mutate(n = n*x3pAveraged_binarized$header.info$incrementY*1e6) %>%
  ggplot(aes(x = n)) +
  geom_histogram(fill = "grey50",colour = "black",
                 binwidth =  7.37364) +
  theme_bw() +
  scale_y_continuous(breaks = 1:9,limits = c(0,6.2)) +
  coord_cartesian(expand = FALSE,xlim = c(0,350)) +
  labs(x = expression(Region~Size~'('~micron^2~')'),
       y = "Number of regions") +
  theme(axis.title = element_text(size = 12),
        axis.text = element_text(size = 11))
```


```{r,eval=FALSE}
cellBased_knownMatch <- readRDS("data/cellBased_knownMatch.rds")

cell16 <- cellBased_knownMatch %>%
  filter(cellIndex == "1, 6" & direction == "reference_vs_target")

cell16_reference <- cell16$cellHeightValues[[1]]
cell16_target <- cell16$alignedTargetCell[[1]]

cell16_reference$surface.matrix <- cell16_reference$surface.matrix*cell16_reference$cmcR.info$scaleByVal*1e6
cell16_target$surface.matrix <- cell16_target$surface.matrix*cell16_target$cmcR.info$scaleByVal*1e6

x3pAveraged_differences <- x3p_filter(x3p = x3p_elemAverage(cell16_reference,cell16_target),
                            cond = function(x,y,thresh) abs(y) > thresh,
                            y = c({cell16_reference$surface.matrix - cell16_target$surface.matrix}),
                            thresh = 1)

cell16_reference_differences <- x3p_filter(x3p = cell16_reference,
                            cond = function(x,y,thresh) abs(y) > thresh,
                            y = c({cell16_reference$surface.matrix - cell16_target$surface.matrix}),
                            thresh = 1)

cell16_target_differences <- x3p_filter(x3p = cell16_target,
                            cond = function(x,y,thresh) abs(y) > thresh,
                            y = c({cell16_reference$surface.matrix - cell16_target$surface.matrix}),
                            thresh = 1)

x3pPlot(cell16_reference_differences,cell16_target_differences)

cor(c(cell16_reference_differences$surface.matrix),
    c(cell16_target_differences$surface.matrix),use = "pairwise.complete.obs")
```

1. There should be more similarities than differences

> *Ratio between number of similar vs. different observations*

![](images/filteredRatioDiagram.png){width="60%" fig-align="center"}

. . .

Compare to a non-match cell comparison:


```{r,eval=FALSE}
K227iG3_processed <- x3ptools::x3p_read("data/K227iG3.x3p")
K227iG3_processed$surface.matrix[t(as.matrix(K227iG3_processed$mask)) == "#1F376CFF"] <- NA
K227iG3_processed$mask <- NULL

K013sA1_processed <- x3ptools::x3p_read("data/K013sA1_processed.x3p")
K013sA1_processed$surface.matrix <- K013sA1_processed$surface.matrix*1e6
K013sA1_processed$header.info$incrementY <- K013sA1_processed$header.info$incrementY*1e6
K013sA1_processed$header.info$incrementX <- K013sA1_processed$header.info$incrementX*1e6

K013sA1_processed <- x3p_interpolate(K013sA1_processed,resx = K227iG3_processed$header.info$incrementX)

cellBased_nonMatch <- scored::comparison_cellBased(K013sA1_processed,K227iG3_processed,thetas = 0)%>%
  filter(cellIndex == "3, 2") %>%
  select(cellHeightValues,alignedTargetCell)

reference <- cellBased_nonMatch$cellHeightValues[[1]]
target <- cellBased_nonMatch$alignedTargetCell[[1]]

x3pAverage_similarities <- x3p_filter(x3p = x3p_elemAverage(reference,target),
                               cond = function(x,y,thresh) abs(y) <= thresh,
                               y = c({reference$surface.matrix - target$surface.matrix}),
                               thresh = 1)

x3pAverage_differences <- x3p_filter(x3p = x3p_elemAverage(reference,target),
                               cond = function(x,y,thresh) abs(y) > thresh,
                               y = c({reference$surface.matrix - target$surface.matrix}),
                               thresh = 1)


sum(!is.na(x3pAverage_similarities$surface.matrix))
sum(!is.na(x3pAverage_differences$surface.matrix))

x3pPlot(x3pAverage_similarities,x3pAverage_differences)

x3pAverage_similarities %>%
  x3p_to_dataFrame() %>%
  mutate(value = !is.na(value)) %>%
  ggplot(aes(x=x,y=y,fill=value)) +
  geom_raster() +
  theme_void() +
  coord_fixed(expand=FALSE) +
  scale_fill_manual(values = c("grey65","white")) +
  theme(legend.position = "none")

x3pAverage_differences %>%
  x3p_to_dataFrame() %>%
  mutate(value = !is.na(value)) %>%
  ggplot(aes(x=x,y=y,fill=value)) +
  geom_raster() +
  theme_void() +
  coord_fixed(expand=FALSE) +
  scale_fill_manual(values = c("grey65","white")) +
  theme(legend.position = "none")


x3pAveraged_binarized <- x3pAverage_differences
x3pAveraged_binarized$surface.matrix <- !is.na(x3pAveraged_binarized$surface.matrix)

scanFilterLabeled <- imager::label(imager::as.cimg(x3pAveraged_binarized$surface.matrix)) %>%
  as.data.frame() %>%
  mutate(value=factor(value))

imager::label(imager::as.cimg(x3pAveraged_binarized$surface.matrix)) %>%
  as.data.frame() %>%
  filter(value > 0) %>%
  group_by(value) %>%
  tally() %>%
  mutate(n = n*x3pAveraged_binarized$header.info$incrementY) %>%
  summarize(meanSize = mean(n),
            sdSize = sd(n))

imager::label(imager::as.cimg(x3pAveraged_binarized$surface.matrix)) %>%
  as.data.frame() %>%
  filter(value > 0) %>%
  group_by(value) %>%
  tally() %>%
  mutate(n = n*x3pAveraged_binarized$header.info$incrementY) %>%
  ggplot(aes(x = n)) +
  geom_histogram(fill = "grey50",colour = "black",
                 binwidth =  7.37364) +
  theme_bw() +
  scale_y_continuous(breaks = 1:9,limits = c(0,6.2)) +
  coord_cartesian(expand = FALSE,xlim = c(0,350)) +
  labs(x = expression(Region~Size~'('~micron^2~')'),
       y = "Number of regions") +
  theme(axis.title = element_text(size = 12),
        axis.text = element_text(size = 11))

scanFilterLabeled %>%
  df_to_x3p() %>%
  x3p_to_dataFrame() %>%
  ggplot(aes(x=x,y=y,fill=value)) +
  geom_raster() +
  coord_fixed(expand=FALSE) +
  theme_void() +
  scale_x_reverse() +
  # scale_y_reverse() +
  theme(legend.position = "none") +
  scale_fill_manual(values = c("gray65",sample(RColorBrewer::brewer.pal(12,"Paired"),
                                               size = length(unique(scanFilterLabeled$value)) - 1,
                                               replace = TRUE)))

reference_differences <- x3p_filter(x3p = reference,
                               cond = function(x,y,thresh) abs(y) > thresh,
                               y = c({reference$surface.matrix - target$surface.matrix}),
                               thresh = 1)
target_differences <- x3p_filter(x3p = target,
                               cond = function(x,y,thresh) abs(y) > thresh,
                               y = c({reference$surface.matrix - target$surface.matrix}),
                               thresh = 1)

x3pPlot(reference_differences,target_differences)
cor(c(reference_differences$surface.matrix),
    c(target_differences$surface.matrix),use = "pairwise.complete.obs")
```

![](images/filteredRatioDiagram_nonMatch.png){width="60%" fig-align="center"}

## Different Region Size {.smaller}

2. The different regions should be relatively small 

> *Size of the different regions*

![](images/neighborhoodSizeExample.png){width=1000 fig-align="center"}

. . .


Compare to a non-match cell comparison:

![](images/neighborhoodSizeExample_nonMatch.png){width=1000 fig-align="center"}

## Different Region Correlation {.smaller  .scrollable}

3. The surface values of the different regions should follow similar trends

> *Correlation between the different regions of the two scans*

![](images/differenceCorrelationExample.png){width="50%" fig-align="center"}

. . .

Compare to a non-match cell comparison:

![](images/differenceCorrelationExample_nonMatch.png){width="50%" fig-align="center"}

<!-- ## Statistics vs. Similarity Scores -->

<!-- - Useful for predicting the score returned by a comparison algorithm -->

<!-- [Figure showing feature value vs. class probability] -->

# Automatic Cartridge Evidence Scoring (ACES) Algorithm

## Automatic Cartridge Evidence Scoring {.smaller}

![](images/ACES_pipelineDiagram.png){fig-align="center" width=1000}

- Comparison algorithm that pre-processes, compares, and scores two cartridge case scans

. . .

- Computes 19 numerical features for each cartridge case pair

. . .

- Predicts match probability for an unknown cartridge case pair using trained statistical model

::: {.notes}

I will first talk through in broad terms the 19 numerical features that we calculate and then I will share some results that show that the ACES algorithm is good at differentiating between matching and non-matching cartridge case pairs.

:::

## Visual Diagnostic Features {.smaller .scrollable}

- Use visual diagnostic statistics discussed earlier as numerical features

. . .

- Features:

  - From the full scan comparison:

    - Similarities vs. differences ratio
    
    - Average and standard deviation of different region sizes
    
    - Different region correlation
    
  - From cell-based comparison:
  
    - Average and standard deviation of similarities vs. differences ratios
    
    - Average and standard deviation of different region sizes
    
    - Average different region correlation

![](images/featureDensity_visualDiag.png){fig-align="center" width="80%"}

## Registration-based Features {.smaller .scrollable}

- For a matching cartridge case pair...

  - Correlation should be large at the full scan *and* cell levels
  
  - Cells should "agree" on a particular registration

. . .

- Compute summary statistics of full-scan and cell-based registration results

- Features:

  - Correlation from full scan comparison
  
  - Mean and standard deviation of correlations from cell comparisons
  
  - Standard deviation of cell-based registration values (horizontal/vertical translations & rotation)

![](images/featureDensity_registration.png){fig-align="center" width="80%"}

## Density-based Features {.smaller .scrollable}

- For a matching cartridge case pair...

  - Cells should "agree" on a particular registration
  
  - The estimated registrations between the two comparison directions should be opposites

. . .

- Apply Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm to the cell-based registration results [@dbscan;@zhang_convergence_2021]

```{r eval=FALSE}
K013sA1_processed <- x3p_read("data/K013sA1_processed.x3p")
K013sA2_processed <- x3p_read("data/K013sA2_processed.x3p")

refVsTarget_8x8 <- scored::comparison_cellBased(reference = K013sA1_processed,
                                                target = K013sA2_processed,
                                                direction = "one",
                                                numCells = c(8,8),
                                                thetas = 3,
                                                returnX3Ps = TRUE)

cmcPlot_refVsTarget <- cmcR::cmcPlot(reference = K013sA1_processed,
                                    target = K013sA2_processed,
                                    cmcClassifs = refVsTarget_8x8  %>%
                                      mutate(clust = scored::densityBasedClusters(x=x,
                                                                                  y=y,
                                                                                  eps=4,
                                                                                  minPts=5),
                                             originalMethod = ifelse(clust > 0,"CMC","Non-CMC")),
                                    type = "list")

targetVsRef_8x8 <- scored::comparison_cellBased(reference = K013sA2_processed,
                                                target = K013sA1_processed,
                                                direction = "one",
                                                numCells = c(8,8),
                                                thetas = -3,
                                                returnX3Ps = TRUE)

cmcPlot_targetVsRef <- cmcR::cmcPlot(reference = K013sA2_processed,
                                    target = K013sA1_processed,
                                    cmcClassifs = targetVsRef_8x8 %>%
                                      mutate(clust = scored::densityBasedClusters(x=x,
                                                                                  y=y,
                                                                                  eps=4,
                                                                                  minPts=5),
                                             originalMethod = ifelse(clust > 0,"CMC","Non-CMC")),
                                    type = "list")

cmcPlot_refVsTarget$target +
  theme(strip.text = element_blank()) +
  annotate(geom = "text",x = 230,y = 190,label = "K013sA2",size = 6)

cmcPlot_targetVsRef$target +
  theme(strip.text = element_blank()) +
  annotate(geom = "text",x = 230,y = 190,label = "K013sA1",size = 6)

refVsTarget_8x8 %>%
  mutate(direction = "K013sA2 Aligned Cell Translations, Rotation: 3 degrees") %>%
  select(cellIndex,direction,x,y) %>%
  group_by(direction) %>%
  mutate(clust = factor(scored::densityBasedClusters(x=x,
                                                 y=y,
                                                 eps=4,
                                                 minPts=5),
                        labels = c("Noise","Cluster")))%>%
  mutate(x = x*K013sA1_processed$header.info$incrementX*1e6,
         y = y*K013sA1_processed$header.info$incrementX*1e6) %>%
  # summarize(x = max(abs(x)),y = max(abs(y)))
  ggplot(aes(x=x,y=y,colour = clust)) +
  # geom_point(size = 2) +
  geom_jitter(size = 2,width = 10,height = 10,
              alpha = .5) +
  theme_bw() +
  coord_fixed() +
  theme(legend.position = "none") +
  scale_color_manual(values = c("red","blue")) +
  labs(colour = "DBSCAN Result",
       x = "horizontal shift (micron)",
       y = "vertical shift (micron)") +
  facet_wrap(~direction,nrow = 1) +
  scale_x_continuous(limits = c(-530,530),
                     breaks = scales::pretty_breaks()) +
  scale_y_continuous(limits = c(-530,530),
                     breaks = scales::pretty_breaks()) +
  geom_hline(yintercept = 0,linetype = "dashed") +
  geom_vline(xintercept = 0,linetype = "dashed")

targetVsRef_8x8 %>%
  mutate(direction = "K013sA1 Aligned Cell Translations, Rotation: -3 degrees") %>%
  select(cellIndex,direction,x,y) %>%
  mutate(clust = factor(scored::densityBasedClusters(x=x,
                                                 y=y,
                                                 eps=4,
                                                 minPts=5),
                        labels = c("Noise","Cluster")))%>%
  mutate(x = x*K013sA1_processed$header.info$incrementX*1e6,
         y = y*K013sA1_processed$header.info$incrementX*1e6) %>%
  # summarize(x = max(abs(x)),y = max(abs(y)))
  ggplot(aes(x=x,y=y,colour = clust)) +
  geom_jitter(size = 2,width = 10,height = 10,
              alpha = .5) +
  theme_bw() +
  coord_fixed() +
  theme(legend.position = "none") +
  scale_color_manual(values = c("red","blue")) +
  labs(colour = "DBSCAN Result",
       x = "reversed horizontal shift (micron)",
       y = "reversed vertical shift (micron)") +
  facet_wrap(~direction,nrow = 1) +
  scale_x_reverse(limits = c(530,-530),
                     breaks = scales::pretty_breaks()) +
  scale_y_reverse(limits = c(530,-530),
                     breaks = scales::pretty_breaks()) +
  geom_hline(yintercept = 0,linetype = "dashed") +
  geom_vline(xintercept = 0,linetype = "dashed")
```

![](images/densityBasedFeatureExample.png){fig-align="center" width=700}

- Features: 

  - Average cluster size

  - DBSCAN cluster indicator

  - Absolute sum of density-estimated rotations

  - Root sum of squares of the cluster-estimated translations

![](images/featureDensity_densityBased.png){fig-align="center" width="80%"}

## ACES Statistical Model {.smaller}

- Compute 19 features for each pairwise comparison

- Use 510 cartridge cases from @baldwin to fit a *logistic regression* classifier

. . .

- Train random logistic regression using 21,945 pairwise comparisons from 210 scans

  - Classify pairs as a "match" or "non-match" based on estimated match probability
  
  <!-- - Select model that balances false positive and false negative error rates -->
  
  - Explore two optimization criteria:

    - Model that minimizes overall error

    - Model that balances false positive and false negative error rates

. . .

- Test model on 44,850 pairwise comparisons from 300 scans

  - Compute false positive and false negative rates for each model
  
  - Consider distributions of match probabilities for truly matching and non-matching pairs

::: {.notes}

One thing we can get from this model is a predicted probability that the two cartridge cases match

:::


## Test Classification Results {.smaller}

| Source | False Positive (%) | False Negative (%) | Overall Error (%) |
| ------ | -------------: | -------------: | -----------: |
| ACES, Minimum Error | 0.07 | 7.66 | 0.59 |
| ACES, Balanced FP/FN | 1.93 | 4.25 | 2.10 |
| | |
| @baldwin | 1.01 | 0.37 | 0.80 |
. . .

Notes:

- We compare *pair*wise (1 to 1), @baldwin compared quartets (3 to 1)

. . .

- Class imbalance in test data: 3,081 match vs. 41,769 non-match comparisons

  - Natural to achieve higher overall accuracy, lower false positive, and higher false negative rate with more non-match classifications
  
. . .
  
- The "Balanced FP/FN" model was selected based on the training data. The test data classifications aren't guaranteed to also be balanced. 

## Match Probability Distributions {.smaller .scrollable}

- We consider classification accuracy as a means of selecting/comparing models. 

- In practice, the examiner would use the estimated match probability as part of their examination.

![](images/matchProbDotPlot.png){fig-align="center"  width="70%"}



# Conclusions

## Conclusions & Future Work {.smaller}

- Automatic comparison algorithms are useful for obtaining numerical measures of similarity for two pieces of evidence

- Visual diagnostics help explain what happens "under the hood" of comparison algorithms

. . .

- Our visual diagnostic tools aid in understanding each step of a cartridge case comparison algorithm

  - Also useful by themselves to visually compare cartridge case evidence

- The Automatic Cartridge Evidence Scoring (ACES) algorithm shows promise at measuring the similarity between cartridge cases

. . .

- Need additional "stress tests" (different ammunition/firearms, degradation levels, etc.)

- Explore other optimization criteria than balancing FP and FN error rates

## Thank You!

- **impressions** R package for visual diagnostics
  - <https://jzemmels.github.io/impressions/>

- **scored** R package for ACES algorithm
  - <https://jzemmels.github.io/scored/>

- **cartridgeInvestigatR** interactive web application
  - <https://csafe.shinyapps.io/cartridgeInvestigatR/>

## References

::: {#refs}

:::

## Appendix

- Specific firearms in the test set tend to have lower associated match probabilities

![](images/matchProbDotPlot_byBarrel.png){fig-align="center" width="60%"}
